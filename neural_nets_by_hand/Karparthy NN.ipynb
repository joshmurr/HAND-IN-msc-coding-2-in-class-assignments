{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.019520718059613\n",
      "-6.016661205026821\n",
      "-6.015469933355874\n",
      "-5.995412301597359\n",
      "-5.952624179201573\n"
     ]
    }
   ],
   "source": [
    "# Random Local Search\n",
    "# -------------------\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "x = -2\n",
    "y = 3\n",
    "\n",
    "def forwardMultiplyGate(x,y):\n",
    "    return x*y\n",
    "\n",
    "tweak_amount = 0.01\n",
    "best_out = -1000000000\n",
    "best_x = x\n",
    "best_y = y\n",
    "\n",
    "for k in range(100):\n",
    "    x_try = x + tweak_amount * (random.random() * 2 - 1) # random val between -1 and 1\n",
    "    y_try = y + tweak_amount * (random.random() * 2 - 1) # random val between -1 and 1\n",
    "    out = forwardMultiplyGate(x_try, y_try)\n",
    "    if out > best_out:\n",
    "        best_out = out\n",
    "        best_x = x_try\n",
    "        best_y = y_try\n",
    "        print(best_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_derivative =  3.00000000000189\n",
      "y_derivative =  -2.0000000000042206\n",
      "The new output is  -5.87059999999986\n"
     ]
    }
   ],
   "source": [
    "# Numerical Gradient\n",
    "# ------------------\n",
    "\n",
    "def forwardMultiplyGate(x, y):\n",
    "    return x * y\n",
    "\n",
    "x = -2\n",
    "y = 3\n",
    "out = forwardMultiplyGate(x, y)\n",
    "h = 0.0001\n",
    "\n",
    "xph = x + h\n",
    "out2 = forwardMultiplyGate(xph, y)\n",
    "x_derivative = (out2 - out) / h # Change WRT x, divided by the change\n",
    "\n",
    "yph = y + h\n",
    "out3 = forwardMultiplyGate(x, yph)\n",
    "y_derivative = (out3 - out) / h # Change WRT y, divided by the change\n",
    "\n",
    "print(\"x_derivative = \", x_derivative)\n",
    "print(\"y_derivative = \", y_derivative)\n",
    "\n",
    "gradient = [x_derivative, y_derivative]\n",
    "\n",
    "step_size = 0.01\n",
    "\n",
    "x = x + (step_size * x_derivative)\n",
    "y = y + (step_size * y_derivative)\n",
    "out_new = forwardMultiplyGate(x, y)\n",
    "\n",
    "print(\"The new output is \", out_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New output =  -5.8706\n"
     ]
    }
   ],
   "source": [
    "# Analytic Gradient\n",
    "# -----------------\n",
    "\n",
    "x = -2\n",
    "y = 3\n",
    "out = forwardMultiplyGate(x, y)\n",
    "x_gradient = y\n",
    "y_gradient = x\n",
    "\n",
    "step_size = 0.01\n",
    "x += step_size * x_gradient\n",
    "y += step_size * y_gradient\n",
    "out_new = forwardMultiplyGate(x, y)\n",
    "\n",
    "print(\"New output = \", out_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f =  -12\n",
      "f =  -11.5924\n",
      "Gradient: wrt_x =  -3.9999999999906777 , wrt_y =  -3.9999999999906777 , wrt_z =  3.000000000010772\n",
      "Analytic Gradient :  [-4.0, -4.0, 3]\n",
      "Close enough!\n"
     ]
    }
   ],
   "source": [
    "# Circuit\n",
    "# -------\n",
    "\n",
    "def forwardMultiplyGate(a, b):\n",
    "    return a * b\n",
    "\n",
    "def forwardAddGate(a, b):\n",
    "    return a + b\n",
    "\n",
    "def forwardCircuit(x, y, z):\n",
    "    q = forwardAddGate(x, y)\n",
    "    f = forwardMultiplyGate(q, z)\n",
    "    return f\n",
    "\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "q = forwardAddGate(x, y)\n",
    "f = forwardMultiplyGate(q, z)\n",
    "\n",
    "print(\"f = \", f)\n",
    "\n",
    "# Derivative of the MULTIPLY gate\n",
    "derivative_f_wrt_z = q\n",
    "derivative_f_wrt_q = z\n",
    "\n",
    "# Derivative of the ADD gate\n",
    "derivative_q_wrt_x = 1.0\n",
    "derivative_q_wrt_y = 1.0\n",
    "\n",
    "# Chain rule\n",
    "derivative_f_wrt_x = derivative_q_wrt_x * derivative_f_wrt_q\n",
    "derivative_f_wrt_y = derivative_q_wrt_y * derivative_f_wrt_q\n",
    "\n",
    "# Final gradient\n",
    "gradient_f_wrt_xyz = [derivative_f_wrt_x, derivative_f_wrt_y, derivative_f_wrt_z]\n",
    "\n",
    "# Let the inputs respond to the manipulations:\n",
    "step_size = 0.01\n",
    "x = x + step_size * derivative_f_wrt_x\n",
    "y = y + step_size * derivative_f_wrt_y\n",
    "z = z + step_size * derivative_f_wrt_z\n",
    "\n",
    "# New circuit outputs\n",
    "q = forwardAddGate(x, y)\n",
    "f = forwardMultiplyGate(q, z)\n",
    "\n",
    "print(\"f = \", f)\n",
    "\n",
    "# Numerical Gradient CHECK with initial conditions\n",
    "\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "\n",
    "h = 0.0001\n",
    "x_derivative = (forwardCircuit(x+h, y, z) - forwardCircuit(x, y, z)) / h\n",
    "y_derivative = (forwardCircuit(x, y+h, z) - forwardCircuit(x, y, z)) / h\n",
    "z_derivative = (forwardCircuit(x, y, z+h) - forwardCircuit(x, y, z)) / h\n",
    "\n",
    "print(\"Gradient: wrt_x = \", x_derivative, \", wrt_y = \", y_derivative, \", wrt_z = \", z_derivative)\n",
    "print(\"Analytic Gradient : \", gradient_f_wrt_xyz)\n",
    "print(\"Close enough!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Unit:\n",
    "    def __init__(self, value, grad):\n",
    "        self.value = value\n",
    "        self.grad = grad\n",
    "        \n",
    "class MultiplyGate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, u0_in, u1_in):\n",
    "        \"\"\"\n",
    "        FORWARD PASS\n",
    "        \n",
    "        Creates an output unit (u_out) based on the input units\n",
    "        \"\"\"\n",
    "        self.u0_in = u0_in\n",
    "        self.u1_in = u1_in\n",
    "        self.u_out = Unit(u0_in.value * u1_in.value, 0.0) # Initial gradient = 0.0\n",
    "        return self.u_out\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        BACK PROPOGATION\n",
    "        \n",
    "        Adjusts the input gradients based on the outputs gradient\n",
    "        \"\"\"\n",
    "        self.u0_in.grad += self.u1_in.value * self.u_out.grad\n",
    "        self.u1_in.grad += self.u0_in.value * self.u_out.grad\n",
    "        \n",
    "class AddGate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, u0_in, u1_in):\n",
    "        \"\"\"\n",
    "        FORWARD PASS\n",
    "        \n",
    "        Creates an output unit (u_out) based on the input units\n",
    "        \"\"\"\n",
    "        self.u0_in = u0_in\n",
    "        self.u1_in = u1_in\n",
    "        self.u_out = Unit(u0_in.value + u1_in.value, 0.0) # Initial gradient = 0.0\n",
    "        return self.u_out\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        BACK PROPOGATION\n",
    "        \n",
    "        Adjusts the input gradients based on the outputs gradient\n",
    "        \"\"\"\n",
    "        self.u0_in.grad += 1 * self.u_out.grad\n",
    "        self.u1_in.grad += 1 * self.u_out.grad\n",
    "        \n",
    "class SigmoidGate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sig(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def forward(self, u0_in):\n",
    "        \"\"\"\n",
    "        FORWARD PASS\n",
    "        \n",
    "        Creates an output unit (u_out) based on the input units\n",
    "        \"\"\"\n",
    "        self.u0_in = u0_in\n",
    "        self.u_out = Unit(self.sig(self.u0_in.value), 0.0)\n",
    "        return self.u_out\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        BACK PROPOGATION\n",
    "        \n",
    "        Adjusts the input gradients based on the outputs gradient\n",
    "        \"\"\"\n",
    "        s = self.sig(self.u0_in.value)\n",
    "        self.u0_in.grad += (s * (1 - s)) * self.u_out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit output s =  0.8807970779778823\n",
      "[-0.10499358540350662, 0.31498075621051985, 0.10499358540350662, 0.10499358540350662, 0.20998717080701323]\n",
      "Circuit output after one backprop =  0.8825501816218984\n"
     ]
    }
   ],
   "source": [
    "# Input Units\n",
    "\n",
    "a = Unit(1.0, 0.0)\n",
    "b = Unit(2.0, 0.0)\n",
    "c = Unit(-3.0, 0.0)\n",
    "x = Unit(-1.0, 0.0)\n",
    "y = Unit(3.0, 0.0)\n",
    "\n",
    "# Gates\n",
    "mulg0 = MultiplyGate()\n",
    "mulg1 = MultiplyGate()\n",
    "addg0 = AddGate()\n",
    "addg1 = AddGate()\n",
    "sg0 = SigmoidGate()\n",
    "\n",
    "def forwardNeuron():\n",
    "    ax = mulg0.forward(a, x)\n",
    "    by = mulg1.forward(b, y)\n",
    "    axpby = addg0.forward(ax, by)\n",
    "    axpbypc = addg1.forward(axpby, c) # a*x + b*y + c\n",
    "    s = sg0.forward(axpbypc)\n",
    "    \n",
    "    return s\n",
    "    \n",
    "s = forwardNeuron()\n",
    "\n",
    "print(\"Circuit output s = \", s.value)\n",
    "\n",
    "# Computing the GRADIENT (backward func in reverse order to above):\n",
    "\n",
    "s.grad = 1.0 # Initialise gradient to 1\n",
    "sg0.backward()\n",
    "addg1.backward()\n",
    "addg0.backward()\n",
    "mulg1.backward()\n",
    "mulg0.backward()\n",
    "\n",
    "# Make the inputs respond to the computed gradients\n",
    "\n",
    "setp_size = 0.01\n",
    "a.value += step_size * a.grad\n",
    "b.value += step_size * b.grad\n",
    "c.value += step_size * c.grad\n",
    "x.value += step_size * x.grad\n",
    "y.value += step_size * y.grad\n",
    "\n",
    "backprop_gradients = [a.grad, b.grad, c.grad, x.grad, y.grad]\n",
    "\n",
    "print(backprop_gradients)\n",
    "\n",
    "s_new = forwardNeuron()\n",
    "\n",
    "print('Circuit output after one backprop = ', s_new.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10499758359205913, 0.3149447748351797, 0.10498958734506125, 0.10498958734506125, 0.2099711788272618]\n"
     ]
    }
   ],
   "source": [
    "def forwardCircuitFast(a, b, c, x, y):\n",
    "    return 1 / (1 + math.exp(- (a*x + b*y + c)))\n",
    "\n",
    "a = 1\n",
    "b = 2\n",
    "c = -3\n",
    "x = -1\n",
    "y = 3\n",
    "h = 0.0001\n",
    "\n",
    "a_grad = (forwardCircuitFast(a+h, b, c, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "b_grad = (forwardCircuitFast(a, b+h, c, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "c_grad = (forwardCircuitFast(a, b, c+h, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "x_grad = (forwardCircuitFast(a, b, c, x+h, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "y_grad = (forwardCircuitFast(a, b, c, x, y+h) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "\n",
    "FCF_gradient = [a_grad, b_grad, c_grad, x_grad, y_grad]\n",
    "\n",
    "print(FCF_gradient) # VERY similar to the backprop_gradients above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Circuit:\n",
    "    def __init__(self):\n",
    "        self.mulg0 = MultiplyGate()\n",
    "        self.mulg1 = MultiplyGate()\n",
    "        self.addg0 = AddGate()\n",
    "        self.addg1 = AddGate()\n",
    "    \n",
    "    def forward(self, x, y, a, b, c):\n",
    "        ax = self.mulg0.forward(a, x)\n",
    "        by = self.mulg1.forward(b, y)\n",
    "        ax_plus_by = self.addg0.forward(ax, by)\n",
    "        self.ax_plus_by_plus_c = self.addg1.forward(ax_plus_by, c)\n",
    "        return self.ax_plus_by_plus_c\n",
    "    \n",
    "    def backward(self, gradient_top):\n",
    "        self.ax_plus_by_plus_c.grad = gradient_top\n",
    "        self.addg1.backward()\n",
    "        self.addg0.backward()\n",
    "        self.mulg1.backward()\n",
    "        self.mulg0.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.a = Unit(1.0, 0.0)\n",
    "        self.b = Unit(-2.0, 0.0)\n",
    "        self.c = Unit(-1.0, 0.0)\n",
    "        self.circuit = Circuit()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.unit_out = self.circuit.forward(x, y, self.a, self.b, self.c)\n",
    "        return self.unit_out\n",
    "    \n",
    "    def backward(self, label):\n",
    "        self.a.grad = 0.0\n",
    "        self.b.grad = 0.0\n",
    "        self.c.grad = 0.0\n",
    "        \n",
    "        pull = 0.0\n",
    "        \n",
    "        if(label == 1 and self.unit_out.value < 1):\n",
    "            pull = 1\n",
    "        if(label == -1 and self.unit_out.value > -1):\n",
    "            pull = -1\n",
    "        \n",
    "        self.circuit.backward(pull)\n",
    "        \n",
    "        self.a.grad += -self.a.value\n",
    "        self.b.grad += -self.b.value\n",
    "        \n",
    "    def learnFrom(self, x, y, label):\n",
    "        self.forward(x, y)\n",
    "        self.backward(label)\n",
    "        self.parameterUpdate()\n",
    "        \n",
    "    def parameterUpdate(self):\n",
    "        step_size = 0.01\n",
    "        self.a.value += step_size * self.a.grad\n",
    "        self.b.value += step_size * self.b.grad\n",
    "        self.c.value += step_size * self.c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data   = [[1.2, 0.7], [-0.3, -0.5], [3.0, 0.1], [-0.1, -1.0], [-1.0, 1.1], [2.1, -3]]\n",
    "labels = [1, -1, 1, -1, -1, 1]\n",
    "\n",
    "svm = SVM()\n",
    "\n",
    "def evalTrainingAccuracy():\n",
    "    num_correct = 0\n",
    "    for i in range(len(data)):\n",
    "        x = Unit(data[i][0], 0.0)\n",
    "        y = Unit(data[i][1], 0.0)\n",
    "        true_label = labels[i]\n",
    "        \n",
    "        predicted_label = 1 if (svm.forward(x,y).value > 0) else -1\n",
    "        \n",
    "        if(predicted_label == true_label):\n",
    "            num_correct += 1\n",
    "            \n",
    "    return num_correct / len(data)\n",
    "\n",
    "for it in range(10000):\n",
    "    i = math.floor(random.random() * len(data))\n",
    "    x = Unit(data[i][0], 0.0)\n",
    "    y = Unit(data[i][1], 0.0)\n",
    "    label = labels[i]\n",
    "    svm.learnFrom(x,y,label)\n",
    "    \n",
    "    #if(it%1000==0):\n",
    "    #    print(\"Training accuracy at iter {0}: {1}\".format(it, evalTrainingAccuracy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "```\n",
    "Training accuracy at iter 0: 0.6666666666666666\n",
    "Training accuracy at iter 25: 0.6666666666666666\n",
    "Training accuracy at iter 50: 0.8333333333333334\n",
    "Training accuracy at iter 75: 0.8333333333333334\n",
    "Training accuracy at iter 100: 0.8333333333333334\n",
    "Training accuracy at iter 125: 0.8333333333333334\n",
    "Training accuracy at iter 150: 0.8333333333333334\n",
    "Training accuracy at iter 175: 0.8333333333333334\n",
    "Training accuracy at iter 200: 0.8333333333333334\n",
    "Training accuracy at iter 225: 0.8333333333333334\n",
    "Training accuracy at iter 250: 0.8333333333333334\n",
    "Training accuracy at iter 275: 0.8333333333333334\n",
    "Training accuracy at iter 300: 0.8333333333333334\n",
    "Training accuracy at iter 325: 0.8333333333333334\n",
    "Training accuracy at iter 350: 0.8333333333333334\n",
    "Training accuracy at iter 375: 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Training accuracy at iter 0: 0.6666666666666666\n",
    "Training accuracy at iter 1000: 0.8333333333333334\n",
    "Training accuracy at iter 2000: 1.0\n",
    "Training accuracy at iter 3000: 1.0\n",
    "Training accuracy at iter 4000: 0.8333333333333334\n",
    "Training accuracy at iter 5000: 1.0\n",
    "Training accuracy at iter 6000: 0.8333333333333334\n",
    "Training accuracy at iter 7000: 0.8333333333333334\n",
    "Training accuracy at iter 8000: 0.8333333333333334\n",
    "Training accuracy at iter 9000: 0.8333333333333334\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy at iter 0: 0.6666666666666666\n",
      "Training accuracy at iter 1000: 1.0\n",
      "Training accuracy at iter 2000: 1.0\n",
      "Training accuracy at iter 3000: 1.0\n",
      "Training accuracy at iter 4000: 0.8333333333333334\n",
      "Training accuracy at iter 5000: 1.0\n",
      "Training accuracy at iter 6000: 1.0\n",
      "Training accuracy at iter 7000: 1.0\n",
      "Training accuracy at iter 8000: 0.8333333333333334\n",
      "Training accuracy at iter 9000: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Condensed Form:\n",
    "\n",
    "a = 1\n",
    "b = -2\n",
    "c = -1\n",
    "\n",
    "def evalTrainingAccuracy(a, b, c):\n",
    "    num_correct = 0\n",
    "    for i in range(len(data)):\n",
    "        x = data[i][0]\n",
    "        y = data[i][1]\n",
    "        true_label = labels[i]\n",
    "        \n",
    "        score = a*x + b*y + c\n",
    "\n",
    "        predicted_label = 1 if (score > 0) else -1\n",
    "        \n",
    "        \n",
    "        if(predicted_label == true_label):\n",
    "            num_correct += 1\n",
    "    \n",
    "        \n",
    "    return num_correct / len(data)\n",
    "\n",
    "for it in range(10000):\n",
    "    i = math.floor(random.random()*len(data))\n",
    "    x = data[i][0]\n",
    "    y = data[i][1]\n",
    "    label = labels[i]\n",
    "    \n",
    "    score = a*x + b*y + c\n",
    "    pull = 0.0\n",
    "    if label == 1 and score < 1:\n",
    "        pull = 1 \n",
    "    if label == -1 and score > -1:\n",
    "        pull = -1 \n",
    "    \n",
    "    step_size = 0.01\n",
    "    a += step_size * (x*pull-a)\n",
    "    b += step_size * (y*pull-b)\n",
    "    c += step_size * (pull)\n",
    "    \n",
    "    if(it%1000==0):\n",
    "        # print(\"i: {0}, data: {1}, label: {2}\".format(i, data[i], label))\n",
    "        # print(\"score: {0}\".format(score))\n",
    "        # print(\"x: {3}, y: {4}, a: {0}, b: {1}, c: {2}\".format(a, b, c, x, y))\n",
    "        print(\"Training accuracy at iter {0}: {1}\".format(it, evalTrainingAccuracy(a, b, c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy at iter 0: 0.3333333333333333\n",
      "Training accuracy at iter 25: 0.3333333333333333\n",
      "Training accuracy at iter 50: 0.5\n",
      "Training accuracy at iter 75: 0.5\n",
      "Training accuracy at iter 100: 0.6666666666666666\n",
      "Training accuracy at iter 125: 0.8333333333333334\n",
      "Training accuracy at iter 150: 0.6666666666666666\n",
      "Training accuracy at iter 175: 0.8333333333333334\n",
      "Training accuracy at iter 200: 1.0\n",
      "Training accuracy at iter 225: 0.8333333333333334\n",
      "Training accuracy at iter 250: 1.0\n",
      "Training accuracy at iter 275: 0.8333333333333334\n",
      "Training accuracy at iter 300: 0.8333333333333334\n",
      "Training accuracy at iter 325: 0.8333333333333334\n",
      "Training accuracy at iter 350: 1.0\n",
      "Training accuracy at iter 375: 1.0\n"
     ]
    }
   ],
   "source": [
    "data   = [[1.2, 0.7], [-0.3, -0.5], [3.0, 0.1], [-0.1, -1.0], [-1.0, 1.1], [2.1, -3]]\n",
    "labels = [1, -1, 1, -1, -1, 1]\n",
    "\n",
    "a1 = random.random() - 0.5;\n",
    "b1 = random.random() - 0.5;\n",
    "c1 = random.random() - 0.5;\n",
    "d1 = random.random() - 0.5;\n",
    "a2 = random.random() - 0.5;\n",
    "b2 = random.random() - 0.5;\n",
    "c2 = random.random() - 0.5;\n",
    "d2 = random.random() - 0.5;\n",
    "a3 = random.random() - 0.5;\n",
    "b3 = random.random() - 0.5;\n",
    "c3 = random.random() - 0.5;\n",
    "d3 = random.random() - 0.5;\n",
    "a4 = random.random() - 0.5;\n",
    "b4 = random.random() - 0.5;\n",
    "c4 = random.random() - 0.5;\n",
    "d4 = random.random() - 0.5;\n",
    "\n",
    "def evalTrainingAccuracy(a1, b1, c1, a2, b2, c2, a3, b3, c3, a4, b4, c4, d4):\n",
    "    num_correct = 0\n",
    "    for i in range(len(data)):\n",
    "        x = data[i][0]\n",
    "        y = data[i][1]\n",
    "        true_label = labels[i]\n",
    "        \n",
    "        n1 = max(0, a1*x + b1*y + c1)\n",
    "        n2 = max(0, a2*x + b2*y + c2)\n",
    "        n3 = max(0, a3*x + b3*y + c3)\n",
    "        score = a4*n1 + b4*n2 + c4*n3 + d4\n",
    "\n",
    "        predicted_label = 1 if (score > 0) else -1\n",
    "        \n",
    "        if(predicted_label == true_label):\n",
    "            num_correct += 1\n",
    "    \n",
    "        \n",
    "    return num_correct / len(data)\n",
    "\n",
    "for it in range(400):\n",
    "    i = math.floor(random.random()*len(data))\n",
    "    x = data[i][0]\n",
    "    y = data[i][1]\n",
    "    label = labels[i]\n",
    "    \n",
    "    # Forward Pass\n",
    "    n1 = max(0, a1*x + b1*y + c1)\n",
    "    n2 = max(0, a2*x + b2*y + c2)\n",
    "    n3 = max(0, a3*x + b3*y + c3)\n",
    "    score = a4*n1 + b4*n2 +c4*n3 + d4\n",
    "    \n",
    "    pull = 0\n",
    "    if (label == 1 and score < 1):\n",
    "        pull = 1 \n",
    "    if (label == -1 and score > -1):\n",
    "        pull = -1\n",
    "        \n",
    "    dscore = pull\n",
    "    da4 = n1 * dscore\n",
    "    dn1 = a4 * dscore\n",
    "    db4 = n2 * dscore\n",
    "    dn2 = b4 * dscore\n",
    "    dc4 = n3 * dscore\n",
    "    dn3 = c4 * dscore\n",
    "    dd4 = 1 * dscore\n",
    "    \n",
    "    # Backprop the ReLU nonlinearities in place\n",
    "    \n",
    "    dn3 = 0 if n3 == 0 else dn3\n",
    "    dn2 = 0 if n2 == 0 else dn2\n",
    "    dn1 = 0 if n1 == 0 else dn1\n",
    "    \n",
    "    da1 = x * dn1\n",
    "    db1 = y * dn1\n",
    "    dc1 = dn1\n",
    "    \n",
    "    da2 = x * dn2\n",
    "    db2 = y * dn2\n",
    "    dc2 = dn2\n",
    "    \n",
    "    da3 = x * dn3\n",
    "    db3 = y * dn3\n",
    "    dc3 = dn3\n",
    "    \n",
    "    da1 += -a1\n",
    "    da2 += -a2\n",
    "    da3 += -a3\n",
    "    db1 += -b1\n",
    "    db2 += -b2\n",
    "    db3 += -b3\n",
    "    da4 += -a4\n",
    "    db4 += -b4\n",
    "    dc4 += -c4\n",
    "    \n",
    "    step_size = 0.01;\n",
    "    a1 += step_size * da1\n",
    "    b1 += step_size * db1 \n",
    "    c1 += step_size * dc1\n",
    "    a2 += step_size * da2 \n",
    "    b2 += step_size * db2\n",
    "    c2 += step_size * dc2\n",
    "    a3 += step_size * da3 \n",
    "    b3 += step_size * db3 \n",
    "    c3 += step_size * dc3\n",
    "    a4 += step_size * da4 \n",
    "    b4 += step_size * db4 \n",
    "    c4 += step_size * dc4 \n",
    "    d4 += step_size * dd4\n",
    "    \n",
    "    if(it%25==0):\n",
    "        # print(\"i: {0}, data: {1}, label: {2}\".format(i, data[i], label))\n",
    "        # print(\"score: {0}\".format(score))\n",
    "        # print(\"x: {3}, y: {4}, a: {0}, b: {1}, c: {2}\".format(a, b, c, x, y))\n",
    "        # print(dn3, dn2, dn1)\n",
    "        print(\"Training accuracy at iter {0}: {1}\".format(it, evalTrainingAccuracy(a1, b1, c1, a2, b2, c2, a3, b3, c3, a4, b4, c4, d4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Training accuracy at iter 0: 0.3333333333333333\n",
    "Training accuracy at iter 25: 0.3333333333333333\n",
    "Training accuracy at iter 50: 0.5\n",
    "Training accuracy at iter 75: 0.5\n",
    "Training accuracy at iter 100: 0.6666666666666666\n",
    "Training accuracy at iter 125: 0.8333333333333334\n",
    "Training accuracy at iter 150: 0.6666666666666666\n",
    "Training accuracy at iter 175: 0.8333333333333334\n",
    "Training accuracy at iter 200: 1.0\n",
    "Training accuracy at iter 225: 0.8333333333333334\n",
    "Training accuracy at iter 250: 1.0\n",
    "Training accuracy at iter 275: 0.8333333333333334\n",
    "Training accuracy at iter 300: 0.8333333333333334\n",
    "Training accuracy at iter 325: 0.8333333333333334\n",
    "Training accuracy at iter 350: 1.0\n",
    "Training accuracy at iter 375: 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
